[Q] memory , disk  requirement to build syntaxNet ?

//=== https://github.com/tensorflow/models/tree/master/syntaxnet
https://github.com/tensorflow/models/tree/master/syntaxnet

git clone --recursive https://github.com/tensorflow/models.git


docker
https://github.com/brianlow/syntaxnet-docker

https://github.com/saturnism/syntaxnet-docker
https://github.com/saturnism/syntaxnet-docker/blob/master/Dockerfile

 Unable to locate package openjdk-8-jdk


sudo add-apt-repository ppa:openjdk-r/ppa
sudo apt-get update
sudo apt-get install openjdk-8-jdk



http://techappro.blogspot.tw/2016/05/how-to-try-new-google-syntaxnet-model.html

1. Install JDK 8Ubuntu Trusty (14.04 LTS). 
OpenJDK 8 is not available on Trusty. To install Oracle JDK 8:
$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update$ sudo apt-get install oracle-java8-installer
Note: You might need to sudo apt-get install software-properties-common if you don't have the add-apt-repository command. See here.

Ubuntu Wily (15.10). To install OpenJDK 8:
$ sudo apt-get install openjdk-8-jdk2. 

Install required packages
$ sudo apt-get install pkg-config zip g++ zlib1g-dev unzip

//=== https://github.com/tensorflow/models/tree/master/syntaxnet

 We are excited to share the fruits of our research with the broader community by releasing 
 SyntaxNet, 
 an open-source neural network framework for TensorFlow that provides a foundation for Natural Language Understanding (NLU) systems. Our release

 * Parsey McParseface, an English parser(using SyntaxNet?) that we have trained for you, 
 and that you can use to analyze English text.
 
 
 Andor et al. (2016)* is simply a SyntaxNet model with a larger beam and network. 
 For futher information on the datasets, see that paper under the section "Treebank Union".
 
 
//=== https://spacy.io/blog/syntaxnet-in-context

""" BY MATTHEW HONNIBAL ON MAY 13, 2016
Yesterday, Google open sourced their Tensorflow-based dependency parsing library, SyntaxNet.

syntaxNet gives access to a line of neural network parsing models published by Google researchers over the last two years. 
 
 
SyntaxNet provides an important module in a natural language processing (NLP) pipeline such as spaCy.
 
//=== spaCy.io
  we just implement one – the best one. 
  When better algorithms are developed, we can update the library 
  without breaking your code or bloating the API. 
  This approach makes spaCy both easier and more powerful than a pluggable architecture. 
  
other NLP libraries rely on sentence detection as a pre-process, spaCy reads the whole document at once, making it much more robust to informal and poorly formatted text.  

//===  food chain
Within this larger value chain, SyntaxNet is a fairly low-level technology. 
It's like an improved drill bit. By itself, it doesn't give you any oil — 
 - and oil, by itself, doesn't give you any energy or plastics, 
 - and energy and plastics by themselves don't give you any work or any products. 
 


//=== 
*** I think that syntactic parsing is a bottle-neck technology in NLP, and that 
the last 4 or 5 years of improvements to this technology will have outsize impacts. 

 Stanford CoreNLP algorithm is almost identical in design, but not in detail. 
 The parsing model used by spaCy is also similar. 
Conceptually, the contribution of the SyntaxNet work is pretty subtle. 
It's mostly about careful experimentation, tuning, and refinement. 

* The neural network models that make SyntaxNet tick have also 
 opened up a rich landscape of sexier ideas, and researchers are busily exploring them.

[ex of ambiguity] They ate the pizza with anchovies.


* SyntaxNet is a library for training and running syntactic dependency parsing models. 
One model ( Parsey McParseface ) that it provides offers a particularly good speed/accuracy trade-off. 

On the time-honoured benchmark for this task, 
Parsey McParseface achieves over 94% accuracy, at around 600 words per second. 
On the same task, spaCy achieves 92.4%, at around 15,000 words per second. 
The extra accuracy might not sound like much, but for applications, it's likely to be pretty significant.

* the difference to a baseline predictor, rather than the absolute accuracy. 
A model that predicts that the weather will be the same as yesterday will often be accurate, 
but it's not adding any value. 

* difference, variation, diversity, versatile,　delta

昨非 今是
there are lots of ideas that didn't work yesterday, that are suddenly becoming very viable.





* excited that there's a very clear way forward, given the design of the Parsey McParseface model.
, i.e. to build a bridge between Parsey McParseface and spaCy, so that you can use the more accurate model with the sweeter spaCy API. 
However ...