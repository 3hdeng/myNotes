https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html

what MNIST is,
what softmax (multinomial logistic) regression is,


MNIST is a simple computer vision "dataset". 
It consists of images of handwritten digits 
It also includes labels for each image, telling us which digit it is.

use TensorFlow
to train a model to look at images and predict what digits they are

***  to dip a toe into using TensorFlow. 
     - to start with a very simple model, called a Softmax Regression.

* The MNIST data is hosted on Yann LeCun's website. 

//=== MNIST data download by input_data.py
https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/mnist/input_data.py

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

The downloaded data is split into three parts,
 55,000 data points of training data (mnist.train), 
 10,000 points of test data (mnist.test), and 
 5,000 points of validation data (mnist.validation). 

every MNIST data point has two parts: 
 an image of a handwritten digit and 
 a corresponding label. 
we will call the images "xs" and the labels "ys".

Each image is 28 pixels by 28 pixels. --> 784

The result is that mnist.train.images is 
a tensor (an n-dimensional array) with a shape of [55000, 784]. 
Each entry in the tensor is the pixel intensity between 0 and 1, 
for a particular pixel in a particular image.
(i,j) --> k
0<= i <55000
0<= j < 784
0<= k < 1


The corresponding labels in MNIST are numbers between 0 and 9, 
represented by one-hot vector
0=[1, 0,0,0, 0,0,0, 0,0,0]
1=[0, 1,0,0, 0,0,0, 0,0,0]
2=[0, 0,1,0, 0,0,0, 0,0,0]
...
9=[0, 0,0,0, 0,0,0, 0,0,1]

//=== Softmax Regressions
If you want to assign probabilities to an object being one of several different things, 
softmax is the thing to do. 
Even later on, when we train more sophisticated models, the final step will be a layer of softmax.



//=== Implementing the Regression

To do efficient numerical computing in Python, we typically use libraries like NumPy that do expensive operations such as matrix multiplication outside Python, using highly efficient code implemented in another language. Unfortunately, there can still be a lot of overhead from switching back to Python every operation. This overhead is especially bad if you want to run computations on GPUs or in a distributed manner, where there can be a high cost to transferring data.

to avoid this overhead(switch to outside lib, compute, switch back to python ). 
Instead of running a single expensive operation independently from Python, 
TensorFlow lets us describe a graph of interacting operations that run entirely outside Python.


To use TensorFlow, we need to import it.

import tensorflow as tf


x = tf.placeholder(tf.float32, [None, 784])
x isn't a specific value. It's a placeholder, 
a value that we'll input when we ask TensorFlow to run a computation. 

We want to be able to input any number of MNIST images, 
each flattened into a 784-dimensional vector. 

We represent this as a 2-D tensor of floating-point numbers, 
with a shape [None, 784]. (Here None means that a dimension can be of any length.)

We also need the weights and biases for our model.
A Variable is a modifiable tensor that lives in TensorFlow's graph of interacting operations.


W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
We create these Variables by giving tf.Variable the initial value of the Variable: in this case, we initialize both W and b as tensors full of zeros. 


Notice that W has a shape of [784, 10] because we want to multiply the 784-dimensional image vectors by it to produce 10-dimensional vectors of evidence for the difference classes

b has a shape of [10] so we can add it to the output.

y = tf.nn.softmax(tf.matmul(x, W) + b)
First, we multiply x by W with the expression tf.matmul(x, W). This is flipped from when we multiplied them in our equation, where we had Wx, as a small trick to deal with x being a 2D tensor with multiple inputs. We then add b, and finally apply tf.nn.softmax.



